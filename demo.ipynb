{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762aa57b-12c8-41ce-98d0-e1593887ca8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.faiss import FAISS\n",
    "import pypdf\n",
    "import pathlib\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from knowledge_gpt.embeddings import OpenAIEmbeddings\n",
    "from datetime import datetime\n",
    "from app import db, Article, app, OPENAI_KEY\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "\n",
    "\n",
    "def parse_pdf(file):\n",
    "    pdf = pypdf.PdfReader(file)\n",
    "    output = []\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        # Merge hyphenated words\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        # Fix newlines in the middle of sentences\n",
    "        text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "        # Remove multiple newlines\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "\n",
    "        output.append(text)\n",
    "\n",
    "    return output\n",
    "\n",
    "def text_to_docs(text, fname):\n",
    "    \"\"\"Converts a string or list of strings to a list of Documents\n",
    "    with metadata.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Take a single string as one page\n",
    "        text = [text]\n",
    "    page_docs = [Document(page_content=page) for page in text]\n",
    "\n",
    "    # Add page numbers as metadata\n",
    "    for i, doc in enumerate(page_docs):\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "    # Split pages into chunks\n",
    "    doc_chunks = []\n",
    "\n",
    "    for doc in page_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\"],\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "            )\n",
    "            # Add sources a metadata\n",
    "            doc.metadata[\"source\"] = f\"{fname} {doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "            doc.metadata[\"file\"] = fname\n",
    "            doc_chunks.append(doc)\n",
    "    return doc_chunks\n",
    "\n",
    "\n",
    "def process(path):\n",
    "    with path.open(\"rb\") as f:\n",
    "        txts = parse_pdf(f)\n",
    "        return text_to_docs(txts, path.stem)\n",
    "                   \n",
    "\n",
    "def summarize_article(doc):\n",
    "    \"\"\" doc is the output from parse\n",
    "    \"\"\"\n",
    "    llm = OpenAI(temperature=0)\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    return chain.run(doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_index(paths, index_file):\n",
    "    docs = [doc for doccol in [process(out) for out in paths] for doc in doccol]\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_key=OPENAI_KEY,\n",
    "    )  # type: ignore\n",
    "\n",
    "    index = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    index.save_local(index_file)\n",
    "\n",
    "    \n",
    "\n",
    "def add_article_to_sqlite(path, summary):\n",
    "    \"\"\" add to sqlite db\n",
    "    \"\"\"\n",
    "    pdf = pypdf.PdfReader(path)\n",
    "    meta = pdf.metadata    \n",
    "\n",
    "    with app.app_context():\n",
    "        # Create a new article object\n",
    "        article = Article(\n",
    "            path=str(path),\n",
    "            summary=summary,\n",
    "            author = meta.author,\n",
    "            creator = meta.creator,\n",
    "            producer = meta.producer,\n",
    "            subject = meta.subject,\n",
    "            title = meta.title,\n",
    "        )\n",
    "        try:\n",
    "            db.session.add(article)\n",
    "            db.session.commit()\n",
    "        except IntegrityError:\n",
    "            db.session.rollback()\n",
    "            print(f\"An article with this path already exists. ({first_doc})\")\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "first_doc, *_ = paths = list(pathlib.Path(\"data\").glob(\"*.pdf\"))\n",
    "index_file = \"tjupyter-faiss\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_KEY,\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6568abe-6985-497a-85bd-720201182160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## create FAISS db\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m, in \u001b[0;36mcreate_index\u001b[0;34m(paths, index_file)\u001b[0m\n\u001b[1;32m     97\u001b[0m docs \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m doccol \u001b[38;5;129;01min\u001b[39;00m [process(out) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m paths] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m doccol]\n\u001b[1;32m     99\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(\n\u001b[1;32m    100\u001b[0m     openai_api_key\u001b[38;5;241m=\u001b[39mOPENAI_KEY,\n\u001b[1;32m    101\u001b[0m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m index\u001b[38;5;241m.\u001b[39msave_local(index_file)\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/langchain/vectorstores/base.py:307\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    306\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/langchain/vectorstores/faiss.py:426\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m    409\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m    428\u001b[0m         texts,\n\u001b[1;32m    429\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    434\u001b[0m     )\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/knowledge_gpt/embeddings.py:104\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_func(text, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_model_name)\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts\n\u001b[1;32m    107\u001b[0m     ]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m responses\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/knowledge_gpt/embeddings.py:105\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     responses \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts\n\u001b[1;32m    107\u001b[0m     ]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m responses\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## create FAISS db\n",
    "create_index(paths, index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bb93e9-8bbc-4386-b5f5-5805794990a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 314.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/s41467-023-36502-3.pdf already in db. Skipping...\n",
      "data/6637898.pdf already in db. Skipping...\n",
      "data/Benzeneco-reactionwithmethanolanddimethyletheroverzeoliteandzeotypecatalystsEvidenceofparallelreactionpathstotolueneanddiphe.pdf already in db. Skipping...\n",
      "data/InsitumonitoringofTiO2BanatasenanoparticleformationandapplicationinLi-ionandNa-ionbatteries.pdf already in db. Skipping...\n",
      "data/InsitustudiesofNOreductionbyH2overPtusingsurfaceX-raydiffractionandtransmissionelectronmicroscopy.pdf already in db. Skipping...\n",
      "data/LocationofCoandNipromoteratomsinmulti-layerMoS2nanocrystalsforhydrotreatingcatalysis.pdf already in db. Skipping...\n",
      "data/molecules-27-08578-v2.pdf already in db. Skipping...\n",
      "data/Morphology-inducedshapeselectivityinzeolitecatalysis.pdf already in db. Skipping...\n",
      "data/nwac045.pdf already in db. Skipping...\n",
      "data/Pandiangan_2019_J._Phys. _Conf._Ser._1338_012015.pdf already in db. Skipping...\n",
      "data/Wang_NanoEnergy_2018.pdf already in db. Skipping...\n",
      "Summary for data/00614.pdf failed! Skipping...\n",
      "data/Cage-basedsmall-porecatalystsforNH3-SCRpreparedbycombiningbulkyorganicstructuredirectingagentswithmodifiedzeolitesasreagents.pdf already in db. Skipping...\n",
      "data/FacileandbenignconversionofsucrosetofructoseusingzeoliteswithbalancedBrønstedandLewisacidity.pdf already in db. Skipping...\n",
      "data/FundamentalchemistryofV-SCRcatalystsatelevatedtemperatures.pdf already in db. Skipping...\n",
      "data/Importance-of-the-Cu-oxidation-state-for-the-SO2poisoning-of-a-CuSAPO34-catalyst-in-the-NH3SCR-reaction.pdf already in db. Skipping...\n",
      "data/Newinsightsintocatalystdeactivationandproductdistributionofzeolitesinthemethanol-to-hydrocarbons(MTH)reactionwithmethanoland.pdf already in db. Skipping...\n",
      "data/QuantitativeNMRApproachtoOptimizetheFormationofChemicalBuildingBlocksfromAbundantCarbohydrates.pdf already in db. Skipping...\n",
      "data/Solidoxidefuelcellspoweredbybiomassgasificationforhighefficiencypowergeneration.pdf already in db. Skipping...\n",
      "data/StructuredeactivationrelationshipsinzeolitesduringthemethanoltohydrocarbonsreactionComplementaryassessmentsofthecokecontent.pdf already in db. Skipping...\n",
      "data/Time-andspace-resolvedstudyofthemethanoltohydrocarbons(MTH)reaction-influenceofzeolitetopologyonaxialdeactivationpatterns.pdf already in db. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create sqlite db\n",
    "from tqdm import tqdm\n",
    "\n",
    "for path in tqdm(paths):\n",
    "\n",
    "    with app.app_context():\n",
    "        specific_article = Article.query.filter_by(path=str(path)).first()\n",
    "        if specific_article:\n",
    "            print(f\"{path} already in db. Skipping...\")\n",
    "            continue\n",
    "    \n",
    "\n",
    "    doc = process(path)\n",
    "    try:\n",
    "        summary = summarize_article(doc)\n",
    "    except:\n",
    "        print(f\"Summary for {path} failed! Skipping...\")\n",
    "        continue\n",
    "    add_article_to_sqlite(path, summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82c686d-ec38-47e2-b74d-4c39b5ee9610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with app.app_context():\n",
    "    articles = list(Article.query.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2b81c6-80c1-4012-99b2-23bbdfd5b124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState at 0x7faa75643940>,\n",
       " 'path': 'data/s41467-023-36502-3.pdf',\n",
       " 'author': 'Monica J. Mendoza-Castro',\n",
       " 'producer': 'iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)',\n",
       " 'title': 'Tunable hybrid zeolites prepared by partial interconversion',\n",
       " 'creator': 'Springer',\n",
       " 'summary': ' This article discusses the synthesis of hybrid zeolites, which are superior catalysts made of building units of different zeolite types. It covers the synthesis of zeolites via interzeolite transformations, the rearrangement of atomic configurations during the conversion of FAU zeolite to CHA zeolite, and the synthesis of a highly ordered mesoporous MSU-SBEA/zeolite Beta composite material. It also covers the use of cellulose nanocrystals as hard templates for preparing mesoporous zeolite Y assemblies.',\n",
       " 'id': 1,\n",
       " 'subject': 'Nature Communications, doi:10.1038/s41467-023-36502-3'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d13f42-9dc0-4a7d-a8e7-b387b827d2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = {\n",
    "\"6637898\": \"A Short Review on Synthesis, Characterization, and Applications of Zeolites\",\n",
    "\"Pandiangan_2019_J._Phys. _Conf._Ser._1338_012015\":\"Characteristics and catalytic activity of zeolite-a synthesized from rice husk silica and aluminium metal by sol-gel method\",\n",
    "\"FacileandbenignconversionofsucrosetofructoseusingzeoliteswithbalancedBrønstedandLewisacidity\":\"Facile and benign conversion of sucrose to fructose using zeolites with balanced Brønsted and Lewis acidity\",\n",
    "}\n",
    "\n",
    "with app.app_context():\n",
    "    articles = Article.query.all()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d244067d-9cdc-4655-a6f6-64c9775013ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "pdf = pypdf.PdfReader(\"data/FacileandbenignconversionofsucrosetofructoseusingzeoliteswithbalancedBrønstedandLewisacidity.pdf\")\n",
    "\n",
    "for page in pdf.pages:\n",
    "    print(page.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b82d2d-9b25-4544-b6a1-8cfc06aa9bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_KEY,\n",
    ")  # type: ignore\n",
    "index = FAISS.load_local(index_file, embeddings = embeddings)\n",
    "\n",
    "query = \"what are zeolies?\"\n",
    "\n",
    "docs = index.similarity_search_with_relevance_scores(query, k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5950f50e-3393-4976-bd90-a75c32553aea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     36\u001b[0m index \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(index_file, embeddings \u001b[38;5;241m=\u001b[39m embeddings)\n\u001b[1;32m     39\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat are zeolies?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m LOWER_RELEVANCE_BOUND \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.72\u001b[39m\n\u001b[1;32m     44\u001b[0m docs \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, relevance \u001b[38;5;129;01min\u001b[39;00m docs \u001b[38;5;28;01mif\u001b[39;00m relevance \u001b[38;5;241m>\u001b[39m LOWER_RELEVANCE_BOUND]\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/langchain/vectorstores/base.py:131\u001b[0m, in \u001b[0;36mVectorStore.similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_relevance_scores\u001b[39m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    113\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    114\u001b[0m     k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    116\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs and relevance scores in the range [0, 1].\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    0 is dissimilar, 1 is most similar.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        List of Tuples of (doc, similarity_score)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     docs_and_similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_similarity_search_with_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    135\u001b[0m         similarity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _, similarity \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities\n\u001b[1;32m    137\u001b[0m     ):\n\u001b[1;32m    138\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance scores must be between\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 0 and 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_and_similarities\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/langchain/vectorstores/faiss.py:532\u001b[0m, in \u001b[0;36mFAISS._similarity_search_with_relevance_scores\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_score_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize_score_fn must be provided to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m FAISS constructor to normalize scores\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[0;32m--> 532\u001b[0m docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(doc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance_score_fn(score)) \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/langchain/vectorstores/faiss.py:224\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    214\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(embedding, k)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/knowledge_gpt/embeddings.py:119\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m/net/dkcphapp003rd/home/topsoe/jikr/projects/knowledge_gpt/.venv/lib/python3.10/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "from knowledge_gpt.prompts import STUFF_PROMPT\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from knowledge_gpt.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def get_answer(docs, query):\n",
    "    \"\"\"Gets an answer to a question from a list of Documents.\"\"\"\n",
    "    # Get the answer\n",
    "    \n",
    "    if len(docs) == 0:\n",
    "        return {\"output_text\":\"I found no relevant articles for that question...\\nSOURCES:\"}\n",
    "\n",
    "    chain = load_qa_with_sources_chain(\n",
    "        OpenAI(\n",
    "            temperature=0, openai_api_key=OPENAI_KEY,\n",
    "        ),  # type: ignore\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=STUFF_PROMPT,\n",
    "    )\n",
    "\n",
    "    # Cohere doesn't work very well as of now.\n",
    "    # chain = load_qa_with_sources_chain(\n",
    "    #     Cohere(temperature=0), chain_type=\"stuff\", prompt=STUFF_PROMPT  # type: ignore\n",
    "    # )\n",
    "    answer = chain(\n",
    "        {\"input_documents\": docs, \"question\": query}, return_only_outputs=True\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_KEY,\n",
    ")  # type: ignore\n",
    "\n",
    "index = FAISS.load_local(index_file, embeddings = embeddings)\n",
    "\n",
    "\n",
    "query = \"what are zeolites?\"\n",
    "\n",
    "docs = index.similarity_search_with_relevance_scores(query, k=15)\n",
    "\n",
    "LOWER_RELEVANCE_BOUND = 0.72\n",
    "docs = [doc for doc, relevance in docs if relevance > LOWER_RELEVANCE_BOUND]\n",
    "\n",
    "answer = get_answer(docs, query)\n",
    "\n",
    "answer_txt, sources_txt = answer[\"output_text\"].split(\"SOURCES\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1e75a04-57c1-4e06-ba35-1009fcf3ddd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': molecules-27-08578-v2 2-4, 6637898 1-2, molecules-27-08578-v2 11-2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdd04a29-33ae-4f7f-9fe0-4b2bf927709d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pandiangan_2019_J._Phys. _Conf._Ser._1338_012015',\n",
       " 'Cage-basedsmall-porecatalystsforNH3-SCRpreparedbycombiningbulkyorganicstructuredirectingagentswithmodifiedzeolitesasreagents',\n",
       " 'molecules-27-08578-v2',\n",
       " '6637898']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_paths = list({doc.metadata[\"file\"] for doc in docs})\n",
    "partial_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "936f14dd-fc7d-4005-935f-272e7241488b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Article('None', 'data/6637898.pdf'),\n",
       " Article('A Comprehensive Review on Zeolite Chemistry for Catalytic Conversion of Biomass/Waste into Green Fuels', 'data/molecules-27-08578-v2.pdf'),\n",
       " Article('None', 'data/Pandiangan_2019_J._Phys. _Conf._Ser._1338_012015.pdf'),\n",
       " Article('Cage-based small-pore catalysts for NH3-SCR prepared by combining bulky organic structure directing agents with modified zeolites as reagents', 'data/Cage-basedsmall-porecatalystsforNH3-SCRpreparedbycombiningbulkyorganicstructuredirectingagentswithmodifiedzeolitesasreagents.pdf')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import or_\n",
    "\n",
    "with app.app_context():\n",
    "    matching_articles = Article.query.filter(or_(Article.path.like('%' + partial_path + '%') for partial_path in partial_paths)).all()\n",
    "\n",
    "matching_articles        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6babf70-5eca-473e-90a7-b144c2322153",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk #0:\n",
      "Introduction\n",
      "\n",
      "chunk #1:\n",
      "35This set-up allows ultrahigh vacuum (UHV) sample preparation followed by in situ high-pressure, high-temperature gas exposure, during SXRD or grazing-incidence small-angleX-ray scattering (GISAXS) experiments with simultaneous reactivitymeasurements viagas product analysis with a quadrupole mass spectrometer. The Pt(110) sample, which was spark cut and polished to 0.1 1from the (110) plane (Surface Preparation Laboratory), was cleaned by repeated cycles of argon ion bombardment and annealing in UHV, resulting in a (1 /C23) missing-row 36 reconstructed surface, exposing narrow (111) facets. The (1 /C23) structure could be stabilized by carbon segregated from thebulk, 37which was not removed completely during cleaning cycles as the sample was not annealed in oxygen\n",
      "\n",
      "chunk #2:\n",
      "Downloaded from https://academic.oup.com/nsr/article/9/9/nwac045/6545815 by guest on 22 April 2023\n",
      "\n",
      "chunk #3:\n",
      "Bands in this region have been linked to both a Fermi resonance phenomenon and the formation of adducts between hydroxyls on partial extra framework Al species and CO [70–72] . Given the apparent correspondence between the disappearance of the band at 3651 cm/C01 and the appearance of the band at 3483 cm/C01, an assignment to partial extra framework Al becomes likely. Importantly, at higher loadings of CO, all strongly acidic hydroxyls are consumed by interaction with CO. In the CO stretching region, bands at 2172 and 2163 cm/C01appear at low loadings\n",
      "\n",
      "chunk #4:\n",
      "Page13of18Downloaded from https://academic.oup.com/nsr/article/9/9/nwac045/6545815 by guest on 22 April 2023\n"
     ]
    }
   ],
   "source": [
    "formatted_chunks = \"\"\"\\n\\n\"\"\".join([f\"chunk #{i}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "print(formatted_chunks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a084563-c0a9-4584-9c23-8ccd2e187070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but that is not a document snippet. Can you please provide a document snippet for me to evaluate?\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=OPENAI_KEY,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"\"\"You are a helpful assistant who helps sort document\n",
    "chunks and decide if they are relevant for a specific query. Your output is a\n",
    "relevance score from 0-10 where 0 means not relevant at all and \n",
    "\n",
    "Here are some examples of how you work:\n",
    "\n",
    "QUERY:\n",
    "how large is an elephant\n",
    "\n",
    "chunk #0:\n",
    "Introduction\n",
    "\n",
    "chunk #1:\n",
    "35This set-up allows ultrahigh vacuum (UHV) sample preparation followed by in situ high-pressure,\n",
    "high-temperature gas exposure, during SXRD or grazing-incidence small-angleX-ray scattering (GISAXS)\n",
    "experiments with simultaneous reactivitymeasurements viagas product analysis with a quadrupole mass\n",
    "spectrometer. The Pt(110) sample, which was spark cut and polished to 0.1 1from the (110) plane\n",
    "(Surface Preparation Laboratory), was cleaned by repeated cycles of argon ion bombardment and\n",
    "annealing in UHV, resulting in a (1 /C23) missing-row 36 reconstructed surface, exposing narrow\n",
    "(111) facets. The (1 /C23) structure could be stabilized by carbon segregated from thebulk, 37which\n",
    "was not removed completely during cleaning cycles as the sample was not annealed in oxygen\n",
    "\n",
    "chunk #2:\n",
    "Downloaded from https://academic.oup.com/nsr/article/9/9/nwac045/6545815 by guest on 22 April 2023\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "    \n",
    "    \"\"\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]\n",
    "\n",
    "chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6becbc-1f3f-4c16-bb3c-bb9f37060942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                0.0.181\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61f79d-0a44-4049-aa1f-be91a7ed66e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
